<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LuciBot mehod leverage the imagination capability of off-the-shelf video generation models to extract supervisory signals.">
  <meta name="keywords" content="video generation, data collection, embodied ai, reward generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LuciBot: Automated Robot Policy Learning from Generated Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/new_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://annonymous-user-123.github.io/LuciBot/">
            LuciBot
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LuciBot: Automated Robot Policy Learning from Generated Videos</h1>
          <div class="is-size-5 publication-authors">
            <a href="https://link-to-author-2-profile.com" class="author-block">Xiaowen Qiu</a><sup>1</sup>,
            <a href="https://wangyian-me.github.io" class="author-block">Yian Wang</a><sup>1</sup>,
            <a href="https://caijiting.github.io" class="author-block">Jiting Cai</a><sup>2</sup>,
            <a href="https://acmlczh.github.io" class="author-block">Zhehuan Chen</a><sup>1</sup>,
            <a href="https://chunru-lin.github.io" class="author-block">Chunru Lin</a><sup>1</sup>,
            <a href="https://zswang666.github.io" class="author-block">Tsun-Hsuan Wang</a><sup>3</sup>,
            <a href="https://people.csail.mit.edu/ganchuang/" class="author-block">Chuang Gan</a><sup>1</sup>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Umass Amherst</span>
            <span class="author-block"><sup>2</sup>SJTU</span>
            <span class="author-block"><sup>3</sup>MIT CSAIL</span>
          </div> 

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.09871"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div> 
          
          <p class="is-size-5 has-text-centered" style="margin-top: 10px;">
            We introduce LuciBot, a pipeline that utilizes a general-purpose video generation model to 
            autonomously generate supervision signals for complex embodied tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <img src="./static/images/iccv_teaser_new.png" alt="Teaser Image" style="max-width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">LuciBot</span> utilizes the imagination ability of off-the-shelf video generation model and extract rich supervisory signals for embodied AI tasks.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Automatically generating training supervision for embodied tasks is essential for collecting large-scale data in simulators.
          </p>
          <p>
            While prior works utilize large language models (LLMs) to generate 
            reward code or leverage vision-language models (VLMs) as supervision, 
            these approaches are largely limited to simple tasks with well-defined rewards, 
            such as pick-and-place. This limitation arises because LLMs struggle to describe complex shapes in code, 
            and VLM-based rewards, such as those derived from CLIP, tend to be less precise.
          </p>
          <p>
            To address these challenges, we propose leveraging the imagination capability of off-the-shelf video generation models. 
            Given an initial simulation frame and a textual task description, a video generation model produces a video demonstrating 
            task completion with correct semantics. We then extract rich supervisory signals from the generated video, including 6D pose 
            sequences of objects, 2D segmentations, and estimated depth, to facilitate task learning within the simulation.
          </p>
          <p>
            Our approach significantly enhances supervision quality for complex embodied tasks, 
            expanding the potential for large-scale training in simulators.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">LuciBot Pipeline</h2>
        <div class="publication-image">
          <img src="./static/images/pipeline_v3.png" alt="Pipeline Image" style="max-width: 100%; height: auto;">
        </div>
      </div>
    </div>
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Given a scene configuration in the simulation and a textual task description, 
            we generate a video based on the rendered image and text description, 
            serving as an imagined execution process for completing the task. 
            Supervisory signals are then extracted from this generated video 
            to optimize an action trajectory for task execution.
          </p>
        </div>
      </div>
    </div>
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Pipeline Description</h2>

        <h3 class="title is-4">Task Setting</h3>
        <div class="content has-text-justified">
          <p>
            We evaluate our method on 10 benchmark tasks spanning diverse materials, 
            including fluid, elasto-plastic, granular, elastic, and rigid objects, 
            thanks to the powerful simulator Genesis. These tasks are categorized into deformable tasks, 
            articulated tasks, and rigid tasks.
          </p>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-image">
              <img src="./static/images/Tasks.png" alt="Tasks Image" style="max-width: 100%; height: auto;">
            </div>
          </div>
        </div>
        <br/>
        <h3 class="title is-4">Video Generation and Selection</h3>
        <div class="content has-text-justified">
          <p>
            We render the initial scene by ray-tracing to obtain the following observations: 
            a photorealistic image, ground-truth depth map ,
            and segmentation mask, we then utilized a LLM rewrite task description and the initial 
            image to condition an image-to-video model to generate a video.
            we employ a rejection sampling approach using a large pre-trained 
            vision-language model (VLM) as an evaluator to filter low-quality generated videos. To estimate foreground object poses, we use their 2D
            segmentation and depth information
    
          </p>
        </div>

        <h3 class="title is-4">Perception</h3>
        <div class="content has-text-justified">
          <p>
            Using previous procedure, we generate some candidate videos per task and select the highest-scoring video as the guidance video.
            we then apply a video segmentation model (SAM2) to track foreground objects.
            Next, we apply a depth inpainting model (DepthLab) to estimate per-frame depth maps.
            To estimate foreground object poses, we use their 2D segmentation and depth information.
            Finally, we sample keyframes at a certain frequency.

          </p>
        </div>

        <h3 class="title is-4">Policy Learning</h3>
        <div class="content has-text-justified">
          <p>
            We extract supervisory signals from the generated video to guide training. These signals include:
            2D Mask of the Actuator; 2D Mask of the Target Object; 3D Point Cloud of the Actuator; 3D Point Cloud of the Target Object;
            Contact Information; Affordance.
            We optimize the motion trajectory of the actuator directly in the simulator, 
            abstracting the action sequence into a series of waypoints based on the frequency of selected keyframes. 
            We initialize the action sequence using the actuator's estimated poses from these keyframes and employ 
            a sample-based optimization method, CMA-ES, to refine the trajectory for task completion.
          </p>
        </div>


      </div>
    </div> -->
    
  
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3">Qualitative Results</h2>
        <div class="content">
          <p>
            Here we show the demo videos of our LuciBot method.
          </p>
        </div>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Generated reference videos</h3>
        <div class="content">
          <p>
            LuciBot utilizes a general-purpose video generation model to generate reference videos that 
            conform to semantics and extracts rich supervisory signals from them to facilitate complex 
            embodied tasks. Here, we show the generated videos, including deformable tasks, articulated tasks, and rigid tasks.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/ref_dough.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/ref_butter.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/reference_video_mochi.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/reference_video_sand.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/reference_video_hammer.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/reference_video_liquid.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/ref_cup.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/reference_video_drawer.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/ref_cabinet.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/reference_video_rubber.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>

        <h3 class="title is-4">Optimized Trajectory Videos</h3>
        <div class="content">
          <p>
            We extract supervisory signals from the generated video then optimize the motion trajectory of the actuator directly in the simulator.
            Here we demonstrate the optimized trajectory videos of the actuators.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_dough3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_butter3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_mochi3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_sand3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_hammer3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_water3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_cup3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/act_rubber3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>

        <h3 class="title is-4">Robotics Results</h3>
        <div class="content">
          <p>
            By obtaining the 6 dofs signals, we sample grasping poses based on grasping affordance and employ inverse kinimatics 
            and path-planning algorithms to control the robot arm to grasp the actuator and follow this trajectory.
            We show the demo videos of robots manipulation as follows.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/dough2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/butter2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/mochi2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/sand.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/insertion.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/water.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/rubber.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/cup1.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/cabinet2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/drawer2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <h3 class="title is-4">Real World Results</h3>
        <div class="content">
          <p>
            Beyond rendered images, our method can also be applied to real-world scenarios. Specifically, 
            we first set up a real-world scene, capture an image of it, 
            and generate a video conditioned on the image and task description. 
            We choose two challenging tasks: Stack cups and Pour water, and conduct real world experiment.
            For stacking cups, the robot successfully stacks the white cup inside the red cup. The videos from left to right show the generated reference video, 
            simulation video, and the real world manipulation video. 
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/realcup_gen.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/realcup_sim.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/realcup_real.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="content">
            <p>
              For pouring water, the robot successfully pours the water from the cola can to the bowl. The videos from left to right show the generated reference video, 
              simulation video, and the real world manipulation video. 
            </p>
          </div>
        <div class="columns is-centered">
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/realwater_gen.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/realwater_sim.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="column is-one-third has-text-centered">
            <video width="100%" controls>
              <source src="./static/videos/realwater_real.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
    
        <!-- <h3 class="title is-4">Optimized Trajectory Videos</h3>
        <div class="content">
          <p>
            Our method significantly outperforms all baselines across tasks, 
            as the goal imagined by the video generation model provides 
            a more direct and accurate supervision signal.
          </p>
        </div>
    
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="publication-image">
              <img src="./static/images/quant_results_new1.png" alt="Tasks Image" style="max-width: 100%; height: auto;">
            </div>
          </div>
        </div>
    
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="publication-image">
              <img src="./static/images/quant_results_new2.png" alt="Tasks Image" style="max-width: 100%; height: auto;">
            </div>
          </div>
        </div>
        <br/> -->
    
      </div>
    </div>

  </div>
</section>


<!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
